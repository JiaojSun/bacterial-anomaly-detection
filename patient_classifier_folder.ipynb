{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - loss: 0.0842\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0145\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0225\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0220\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0187\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0141\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0100\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0088\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0089\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0082\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0072\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0068\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0067\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0066\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0060\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0060\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0055\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0055\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0051\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - loss: 0.0051\n"
     ]
    }
   ],
   "source": [
    "# 导入所需库\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "# 加载数据集的函数\n",
    "# def load_images_from_folder(folder, size=(256, 256)):\n",
    "#     images = []\n",
    "#     for filename in os.listdir(folder):\n",
    "#         img = load_img(os.path.join(folder, filename), target_size=size)\n",
    "#         if img is not None:\n",
    "#             images.append(img_to_array(img))\n",
    "#     return np.array(images)\n",
    "\n",
    "def load_images_from_folders(parent_folder, size=(256, 256)):\n",
    "    images = []\n",
    "    for folder in os.listdir(parent_folder):\n",
    "        folder_path = os.path.join(parent_folder, folder)\n",
    "        if os.path.isdir(folder_path):  # 检查这个路径是否为文件夹\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 添加图片格式检查\n",
    "                    img_path = os.path.join(folder_path, filename)\n",
    "                    try:\n",
    "                        img = load_img(img_path, target_size=size)\n",
    "                        images.append(img_to_array(img))\n",
    "                    except Exception as e:\n",
    "                        print(f\"无法加载图片: {img_path}。错误: {e}\")\n",
    "    return np.array(images)\n",
    "\n",
    "# def load_images_from_folders(parent_folder, size=(256, 256)):\n",
    "#     images = []\n",
    "#     for folder in os.listdir(parent_folder):\n",
    "#         folder_path = os.path.join(parent_folder, folder)\n",
    "#         if os.path.isdir(folder_path):  # 检查这个路径是否为文件夹\n",
    "#             for filename in os.listdir(folder_path):\n",
    "#                 img = load_img(os.path.join(folder_path, filename), target_size=size)\n",
    "#                 if img is not None:\n",
    "#                     images.append(img_to_array(img))\n",
    "#     return np.array(images)\n",
    "\n",
    "def create_autoencoder(input_shape=(256, 256, 3)):\n",
    "    input_img = tf.keras.Input(shape=input_shape)\n",
    "    # 编码器部分\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    # 添加Dropout层进行正则化\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    # 编码器的更多层可以在这里加入...\n",
    "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    # 解码器部分\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    # 添加Dropout层进行正则化\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    # 解码器的更多层可以在这里加入...\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    autoencoder = models.Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return autoencoder\n",
    "# 加载数据\n",
    "# train_data_path = 'healfolder'  # 训练数据文件夹路径\n",
    "# test_data_path = 'sickfolder'   # 测试数据文件夹路径\n",
    "train_data_path = 'trainfolder'  # 训练数据文件夹路径\n",
    "\n",
    "# B22-31_1\n",
    "\n",
    "\n",
    "x_train = load_images_from_folders(train_data_path) / 255.0\n",
    "# x_test = load_images_from_folders(test_data_path) / 255.0\n",
    "\n",
    "# 创建自编码器模型\n",
    "autoencoder = create_autoencoder()\n",
    "\n",
    "# 训练模型\n",
    "history = autoencoder.fit(\n",
    "    x_train, x_train, \n",
    "    epochs=20,  # 可能需要更多的训练周期\n",
    "    batch_size=32, \n",
    "    shuffle=True,\n",
    "    # validation_data=(x_test, x_test)  # 使用独立的测试集作为验证数据\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_folders========\n",
      "['trainfolder/B22-25_1', 'trainfolder/B22-26_1']\n",
      "trainfolder/B22-25_1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 570ms/step\n",
      "trainfolder/B22-26_1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 520ms/step\n",
      "losses======\n",
      "[0.13274518, 0.13054638]\n",
      "Calculated threshold: 0.13384456932544708\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "def load_images_from_folder(parent_folder, size=(256, 256)):\n",
    "    print(parent_folder)\n",
    "    images = []\n",
    "    for filename in os.listdir(parent_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 添加图片格式检查\n",
    "            img_path = os.path.join(parent_folder, filename)\n",
    "            try:\n",
    "                # print(img_path)\n",
    "                img = load_img(img_path, target_size=size)\n",
    "                images.append(img_to_array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"无法加载图片: {img_path}。错误: {e}\")\n",
    "    return np.array(images)\n",
    "\n",
    "def calculate_patient_loss(model, folder):\n",
    "    images = load_images_from_folder(folder) / 255.0\n",
    "    reconstructions = model.predict(images)\n",
    "    loss = tf.keras.losses.mae(reconstructions, images)\n",
    "    avg_loss = tf.reduce_mean(loss).numpy()  # 对单个病人文件夹内的所有图片计算平均MAE\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_threshold(model, base_folder):\n",
    "    patient_folders = [os.path.join(base_folder, name) for name in os.listdir(base_folder) \n",
    "                       if os.path.isdir(os.path.join(base_folder, name))]\n",
    "    print('patient_folders========')\n",
    "    print(patient_folders)\n",
    "    losses = [calculate_patient_loss(model, folder) for folder in patient_folders]\n",
    "    print('losses======')\n",
    "    print(losses)\n",
    "    mean_loss = np.mean(losses)\n",
    "    std_loss = np.std(losses)\n",
    "    threshold = mean_loss + 2 * std_loss  # 这里使用均值加两倍标准差\n",
    "    return threshold\n",
    "\n",
    "# 示例用法\n",
    "# autoencoder = create_autoencoder()  # 假设模型已经被训练\n",
    "base_folder = 'trainfolder'  # 假设这是包含所有病人子文件夹的根目录\n",
    "\n",
    "# 计算阈值\n",
    "threshold = calculate_threshold(autoencoder, base_folder)\n",
    "print(\"Calculated threshold:\", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_folders========\n",
      "['baixafolder/B22-69_1', 'baixafolder/B22-73_1']\n",
      "baixafolder/B22-69_1\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 486ms/step\n",
      "baixafolder/B22-73_1\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 461ms/step\n",
      "losses======\n",
      "[0.13032985, 0.17349161]\n",
      "Calculated threshold: 0.15191072\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "def load_images_from_folder(parent_folder, size=(256, 256)):\n",
    "    print(parent_folder)\n",
    "    images = []\n",
    "    for filename in os.listdir(parent_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 添加图片格式检查\n",
    "            img_path = os.path.join(parent_folder, filename)\n",
    "            try:\n",
    "                # print(img_path)\n",
    "                img = load_img(img_path, target_size=size)\n",
    "                images.append(img_to_array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"无法加载图片: {img_path}。错误: {e}\")\n",
    "    return np.array(images)\n",
    "\n",
    "def calculate_patient_loss(model, folder):\n",
    "    images = load_images_from_folder(folder) / 255.0\n",
    "    reconstructions = model.predict(images)\n",
    "    loss = tf.keras.losses.mae(reconstructions, images)\n",
    "    avg_loss = tf.reduce_mean(loss).numpy()  # 对单个病人文件夹内的所有图片计算平均MAE\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_threshold(model, base_folder):\n",
    "    patient_folders = [os.path.join(base_folder, name) for name in os.listdir(base_folder) \n",
    "                       if os.path.isdir(os.path.join(base_folder, name))]\n",
    "    print('patient_folders========')\n",
    "    print(patient_folders)\n",
    "    losses = [calculate_patient_loss(model, folder) for folder in patient_folders]\n",
    "    print('losses======')\n",
    "    print(losses)\n",
    "    mean_loss = np.mean(losses)\n",
    "    std_loss = np.std(losses)\n",
    "    # threshold = mean_loss + 2 * std_loss  # 这里使用均值加两倍标准差\n",
    "    threshold = mean_loss\n",
    "    return threshold\n",
    "\n",
    "# 示例用法\n",
    "# autoencoder = create_autoencoder()  # 假设模型已经被训练\n",
    "# base_folder = 'medfolder'  # 假设这是包含所有病人子文件夹的根目录\n",
    "base_folder = 'baixafolder'\n",
    "\n",
    "\n",
    "# 计算阈值\n",
    "threshold = calculate_threshold(autoencoder, base_folder)\n",
    "print(\"Calculated threshold:\", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_folders========\n",
      "['negfolder/B22-61_1', 'negfolder/B22-150_1', 'negfolder/B22-60_1']\n",
      "negfolder/B22-61_1\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 492ms/step\n",
      "negfolder/B22-150_1\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 492ms/step\n",
      "negfolder/B22-60_1\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 508ms/step\n",
      "losses======\n",
      "[0.1683996, 0.16619869, 0.12776296]\n",
      "Calculated threshold: 0.15412043\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "def load_images_from_folder(parent_folder, size=(256, 256)):\n",
    "    print(parent_folder)\n",
    "    images = []\n",
    "    for filename in os.listdir(parent_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 添加图片格式检查\n",
    "            img_path = os.path.join(parent_folder, filename)\n",
    "            try:\n",
    "                # print(img_path)\n",
    "                img = load_img(img_path, target_size=size)\n",
    "                images.append(img_to_array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"无法加载图片: {img_path}。错误: {e}\")\n",
    "    return np.array(images)\n",
    "\n",
    "def calculate_patient_loss(model, folder):\n",
    "    images = load_images_from_folder(folder) / 255.0\n",
    "    reconstructions = model.predict(images)\n",
    "    loss = tf.keras.losses.mae(reconstructions, images)\n",
    "    avg_loss = tf.reduce_mean(loss).numpy()  # 对单个病人文件夹内的所有图片计算平均MAE\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_threshold(model, base_folder):\n",
    "    patient_folders = [os.path.join(base_folder, name) for name in os.listdir(base_folder) \n",
    "                       if os.path.isdir(os.path.join(base_folder, name))]\n",
    "    print('patient_folders========')\n",
    "    print(patient_folders)\n",
    "    losses = [calculate_patient_loss(model, folder) for folder in patient_folders]\n",
    "    print('losses======')\n",
    "    print(losses)\n",
    "    mean_loss = np.mean(losses)\n",
    "    std_loss = np.std(losses)\n",
    "    # threshold = mean_loss + 2 * std_loss  # 这里使用均值加两倍标准差\n",
    "    threshold = mean_loss\n",
    "    return threshold\n",
    "\n",
    "# 示例用法\n",
    "# autoencoder = create_autoencoder()  # 假设模型已经被训练\n",
    "# base_folder = 'negfolder'  # 假设这是包含所有病人子文件夹的根目录\n",
    "base_folder = 'negfolder'\n",
    "\n",
    "\n",
    "# 计算阈值\n",
    "threshold = calculate_threshold(autoencoder, base_folder)\n",
    "print(\"Calculated threshold:\", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_folders========\n",
      "['highfolder/B22-126_1', 'highfolder/B22-38_1', 'highfolder/B22-37_1']\n",
      "highfolder/B22-126_1\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 511ms/step\n",
      "highfolder/B22-38_1\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 489ms/step\n",
      "highfolder/B22-37_1\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 507ms/step\n",
      "losses======\n",
      "[0.16393185, 0.17423485, 0.17082447]\n",
      "Calculated threshold: 0.16966373\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "def load_images_from_folder(parent_folder, size=(256, 256)):\n",
    "    print(parent_folder)\n",
    "    images = []\n",
    "    for filename in os.listdir(parent_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 添加图片格式检查\n",
    "            img_path = os.path.join(parent_folder, filename)\n",
    "            try:\n",
    "                # print(img_path)\n",
    "                img = load_img(img_path, target_size=size)\n",
    "                images.append(img_to_array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"无法加载图片: {img_path}。错误: {e}\")\n",
    "    return np.array(images)\n",
    "\n",
    "def calculate_patient_loss(model, folder):\n",
    "    images = load_images_from_folder(folder) / 255.0\n",
    "    reconstructions = model.predict(images)\n",
    "    loss = tf.keras.losses.mae(reconstructions, images)\n",
    "    avg_loss = tf.reduce_mean(loss).numpy()  # 对单个病人文件夹内的所有图片计算平均MAE\n",
    "    return avg_loss\n",
    "\n",
    "def calculate_threshold(model, base_folder):\n",
    "    patient_folders = [os.path.join(base_folder, name) for name in os.listdir(base_folder) \n",
    "                       if os.path.isdir(os.path.join(base_folder, name))]\n",
    "    print('patient_folders========')\n",
    "    print(patient_folders)\n",
    "    losses = [calculate_patient_loss(model, folder) for folder in patient_folders]\n",
    "    print('losses======')\n",
    "    print(losses)\n",
    "    mean_loss = np.mean(losses)\n",
    "    std_loss = np.std(losses)\n",
    "    # threshold = mean_loss + 2 * std_loss  # 这里使用均值加两倍标准差\n",
    "    threshold = mean_loss\n",
    "    return threshold\n",
    "\n",
    "# 示例用法\n",
    "# autoencoder = create_autoencoder()  # 假设模型已经被训练\n",
    "base_folder = 'highfolder'  # 假设这是包含所有病人子文件夹的根目录\n",
    "# base_folder = 'data-set/ALTA'\n",
    "\n",
    "# 计算阈值\n",
    "threshold = calculate_threshold(autoencoder, base_folder)\n",
    "print(\"Calculated threshold:\", threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CroppedPaches/B22-106_1\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 517ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-106_1\n",
      "0.16149233\n",
      "CroppedPaches/B22-107_1\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 503ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-107_1\n",
      "0.15857658\n",
      "CroppedPaches/B22-108_1\n",
      "\u001b[1m82/82\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 523ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-108_1\n",
      "0.16035143\n",
      "CroppedPaches/B22-109_1\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 511ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-109_1\n",
      "0.16104965\n",
      "CroppedPaches/B22-110_1\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 509ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-110_1\n",
      "0.15767793\n",
      "CroppedPaches/B22-111_1\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 522ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-111_1\n",
      "0.16236086\n",
      "CroppedPaches/B22-112_1\n",
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 503ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-112_1\n",
      "0.17344996\n",
      "CroppedPaches/B22-126_1\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 513ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-126_1\n",
      "0.15811044\n",
      "CroppedPaches/B22-129_1\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 503ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-129_1\n",
      "0.11849179\n",
      "CroppedPaches/B22-130_1\n",
      "\u001b[1m89/89\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 538ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-130_1\n",
      "0.11359676\n",
      "CroppedPaches/B22-134_1\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 489ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-134_1\n",
      "0.11843738\n",
      "CroppedPaches/B22-136_1\n",
      "\u001b[1m107/107\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 530ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-136_1\n",
      "0.16526534\n",
      "CroppedPaches/B22-20_1\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 528ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-20_1\n",
      "0.16518533\n",
      "CroppedPaches/B22-37_1\n",
      "\u001b[1m36/36\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 534ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-37_1\n",
      "0.16509248\n",
      "CroppedPaches/B22-38_1\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 514ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-38_1\n",
      "0.16852035\n",
      "CroppedPaches/B22-60_1\n",
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 534ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-60_1\n",
      "0.12255007\n",
      "CroppedPaches/B22-61_1\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 526ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-61_1\n",
      "0.1625508\n",
      "CroppedPaches/B22-69_1\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 504ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-69_1\n",
      "0.12578641\n",
      "CroppedPaches/B22-73_1\n",
      "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 481ms/step\n",
      "avg_loss=======\n",
      "CroppedPaches\n",
      "B22-73_1\n",
      "0.16861235\n",
      "Accuracy = 0.3157894736842105\n",
      "Precision = 0.21862348178137653\n",
      "Recall = 0.3157894736842105\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "def load_images_from_folder(parent_folder, size=(256, 256)):\n",
    "    print(parent_folder)\n",
    "    images = []\n",
    "    for filename in os.listdir(parent_folder):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # 添加图片格式检查\n",
    "            img_path = os.path.join(parent_folder, filename)\n",
    "            try:\n",
    "                # print(img_path)\n",
    "                img = load_img(img_path, target_size=size)\n",
    "                images.append(img_to_array(img))\n",
    "            except Exception as e:\n",
    "                print(f\"无法加载图片: {img_path}。错误: {e}\")\n",
    "    return np.array(images)\n",
    "\n",
    "def calculate_patient_loss(model, folder):\n",
    "    images = load_images_from_folder(folder) / 255.0\n",
    "    reconstructions = model.predict(images)\n",
    "    loss = tf.keras.losses.mae(reconstructions, images)\n",
    "    avg_loss = tf.reduce_mean(loss).numpy()  # 对单个病人文件夹内的所有图片计算平均MAE\n",
    "    return avg_loss\n",
    "\n",
    "actual_labels = [0, 1, 1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# 评估病人状态并计算准确度、精确度和召回率\n",
    "def evaluate_patients(model, base_folder, actual_labels):\n",
    "    predictions = []\n",
    "    for folder in sorted(os.listdir(base_folder)):\n",
    "        if os.path.isdir(os.path.join(base_folder, folder)):\n",
    "            avg_loss = calculate_patient_loss(model, os.path.join(base_folder, folder))\n",
    "            # 假设异常的病人文件夹名包含\"abnormal\"\n",
    "            print('avg_loss=======')\n",
    "            print(base_folder)\n",
    "            print(folder)\n",
    "            print(avg_loss)\n",
    "            # predicted_label = 1 if avg_loss > threshold else 0\n",
    "                        # 根据avg_loss的值分配标签\n",
    "            if avg_loss < 0.165:\n",
    "                predicted_label = -1  # NEGATIVA\n",
    "            elif 0.165 <= avg_loss <= 0.17:\n",
    "                predicted_label = 0   # BAIXA\n",
    "            else:\n",
    "                predicted_label = 1   # ALTA\n",
    "            predictions.append(predicted_label)\n",
    "\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(actual_labels, predictions)))\n",
    "    # print(\"Precision = {}\".format(precision_score(actual_labels, predictions, average='weighted')))\n",
    "    # print(\"Recall = {}\".format(recall_score(actual_labels, predictions, average='weighted')))\n",
    "    print(\"Precision = {}\".format(precision_score(actual_labels, predictions)))\n",
    "    print(\"Recall = {}\".format(recall_score(actual_labels, predictions)))\n",
    "\n",
    "    # 生成混淆矩阵\n",
    "    cm = confusion_matrix(actual_labels, predictions)\n",
    "\n",
    "    # 使用Seaborn绘制混淆矩阵图\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# x_test = load_images_from_folders('CroppedPaches') / 255.0\n",
    "x_test = 'CroppedPaches'\n",
    "\n",
    "evaluate_patients(autoencoder, x_test, actual_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
